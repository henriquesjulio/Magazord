# Pipeline de ETL de uma empresa fict√≠cia DataMart.

[![PySpark](https://img.shields.io/badge/PySpark-3.5.0-red)](https://spark.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-17-336791)](https://www.postgresql.org/)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)

Pipeline ETL para processamento de dados de transa√ß√µes comerciais, integrando dados externos com um DataMart em PostgreSQL.

## üìã Vis√£o Geral do Pipeline

1. **Extra√ß√£o**:
   - Dados de clientes, produtos e transa√ß√µes de fontes externas
   - Dados existentes do DataMart PostgreSQL
2. **Transforma√ß√£o**:
   - Normaliza√ß√£o de nomes
   - Valida√ß√£o de tipos de dados
   - C√°lculo de m√©tricas agregadas
   - Tratamento de valores ausentes
3. **Carregamento**:
   - Atualiza√ß√£o das tabelas dimensionais
   - Armazenamento particionado de transa√ß√µes
   - Tabela agregada para an√°lise

## ‚öôÔ∏è Configura√ß√£o Necess√°ria

### Banco de Dados PostgreSQL
```sql
CREATE DATABASE DataMart;
CREATE TABLE clientes(id_cliente INT PRIMARY KEY, nome_cliente VARCHAR(100), email VARCHAR(100), telefone VARCHAR(15));
CREATE TABLE produtos(id_produto INT PRIMARY KEY, nome_produto VARCHAR(100), categoria VARCHAR(50), preco DECIMAL(10,2));
CREATE TABLE transacoes(id_transacao INT PRIMARY KEY, id_cliente INT, id_produto INT, quantidade INT, data_transacao DATE);
CREATE TABLE agregada(id_cliente INT PRIMARY KEY, receita_total DECIMAL(18,2), numero_total_transacoes INT, mais_comprado INT);
```

# üõ†Ô∏è Instala√ß√£o de Depend√™ncias

Instalar JDK 8 no m√≠nimo.

Instalar Hadoop Spark

Instalar JDBC Driver do PostgreSQL:

Baixar postgresql-42.7.5.jar

Colocar em C:/Program Files/PostgreSQL/Driver/

Instalar pacotes Python:
```
bash
Copy
pip install pyspark
```

# üöÄ Execu√ß√£o do Pipeline
```
bash
Copy
spark-submit \
--driver-class-path "C:/Program Files/PostgreSQL/Driver/postgresql-42.7.5.jar" \
--master local[*] \
pipeline.py
```

# üîÑ Fluxo de Transforma√ß√µes
# 1. Normaliza√ß√£o de Dados
python
Copy
# Capitaliza√ß√£o de nomes
```spark.sql('''
SELECT
    id,
    CONCAT(UPPER(SUBSTRING(nome,1,1)), 
    LOWER(SUBSTRING(nome,2))) AS nome
FROM clientes''')
```

# 2. Tratamento de Dados
python
Copy
# Preenchimento de pre√ßos ausentes
```spark.sql('''
SELECT
    COALESCE(preco, AVG(preco)) AS preco
FROM produtos''')
```

# 3. Agrega√ß√£o de M√©tricas
```
python
Copy
# C√°lculo da receita total
spark.sql('''
SELECT
    t.id_cliente,
    SUM(t.quantidade * p.preco) AS receita_total
FROM transacoes t
JOIN produtos p ON t.id_produto = p.id_produto''')
```

# üìä Estrutura Final do DataMart
Tabela	Descri√ß√£o	Particionamento
clientes	Informa√ß√µes de clientes	-
produtos	Cat√°logo de produtos	-
transacoes	Registros de transa√ß√µes di√°rias	Por data_referencia (YYYYMM)
agregada	M√©tricas consolidadas por cliente	-


# ‚ö†Ô∏è Configura√ß√µes Especiais
```
python
Copy
spark = SparkSession.builder \
    .config("spark.sql.shuffle.partitions", "1000") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "8g") \
    .getOrCreate()
```

# üîç Monitoramento
Verificar dados no PostgreSQL ap√≥s execu√ß√£o:
```
sql
Copy
SELECT * FROM agregada ORDER BY receita_total DESC LIMIT 10;
```

# üõë Solu√ß√£o de Problemas Comuns
Erro de Memory Overflow: Aumentar configura√ß√µes de mem√≥ria no SparkSession

Problemas de Conex√£o: Verificar se o servi√ßo PostgreSQL est√° ativo

Formato de Datas: Validar formato das datas nas transa√ß√µes (YYYY-MM-DD)

# üìå Notas Importantes
Particionamento autom√°tico por data de refer√™ncia nas transa√ß√µes

Merge de dados novos com existentes via UNION

Otimiza√ß√£o de escritas usando reparti√ß√£o:
```
python
Copy
tbl_clientes.repartition(100).write.mode('append')...
```

OBS: Os dados de exemplo est√£o dispon√≠veis no reposit√≥rio magazord-plataforma/data_engineer_test
